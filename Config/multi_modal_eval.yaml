# Control
adjust_norm: False
generate: False
prompt_template: "llama3"
stage: 0
weight_sample: False

# Tune Parameter
tune_im_start: False
tune_im_patch: False
tune_rgb_bk: False
tune_rgb_pooler: True

rgb_vision:
  arch: vit_large
  vit_name: google/siglip-so400m-patch14-384
  input_size: [384, 384]
  patch_dropout: 0.
  input_patchnorm: False
  tune_pooler: True
  attn_pooler:
    num_query: 272
    num_attn_heads: 8
    num_layers: 6
    use_moe: True
    num_experts: 4
    num_selects: 2
text:
  path: meta-llama/Meta-Llama-3-8B-Instruct
  hidden_size: 4096
transform:
  input_size: [384, 384]
  rand_aug: rand-m5-n2-mstd0.5-inc1
eval:
  dataset: AID

# training config
dtype: float16
bits: 16
double_quant: True
quant_type: nf4
fp16: True
bf16: False
lora:
  enable: False  # True for stage 2
  lora_r: 128
  lora_alpha: 256
  lora_dropout: 0.05
  lora_bias: none

# optimizer
optimizer: adamw
lr: 0.0002
wd: 0.
epochs: 1
max_grad_norm: 1.0

# scheduler
schedule:
  name: cosine
  min_lr: 0.00002
  warmup_epochs: 100
  warmup_method: linear
  warmup_factor: 0.1
  decay_epochs: 30
  decay_rate: 0.1
  multisteps: []
  gamma: 0.1